{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0222aa9",
   "metadata": {},
   "source": [
    "# Crowdless â€” Crowdness Demo\n",
    "\n",
    "End-to-end notebook for training, exploring, and predicting a **crowdness score** from generic per-area time series JSON files under `./areas_output`.\n",
    "\n",
    "**Input JSON expectation**\n",
    "- Each file: `{ \"area\": \"<name>\", \"items\": [ { \"datetime\": \"...\", \"<field_a>\": <number>, \"<field_b>\": <number> }, ... ] }`\n",
    "- Use `FIELD_MAP` below to map your JSON field names to `feature_x` and `feature_y`. Default mapping aligns with the current test files.\n",
    "\n",
    "**Outputs**\n",
    "- `areas_output/metrics_database.pkl` â€” processed DataFrame for fast lookup\n",
    "- `areas_output/crowdness_model.json` â€” simple, tunable scoring rule parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a50e24",
   "metadata": {},
   "source": [
    "## Data collector (Copernicus Data Space)\n",
    "\n",
    "Creates `collect_s2_catalog.py` in the notebook directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83db9540",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "script = '''\\\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import argparse\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "# import imageio.v2 as imageio # Not needed if not computing real indices\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import hashlib\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "CLIENT_ID = \<enter client id>\"\n",
    "CLIENT_SECRET = \<enter client secret>\"\n",
    "DEFAULT_BBOX = [23.70, 37.95, 23.78, 38.02]\n",
    "DEFAULT_START = \"2016-01-01T00:00:00Z\"\n",
    "DEFAULT_END = \"2025-11-30T23:59:59Z\"\n",
    "AREAS = {\n",
    "    \"Kallithea\": [23.7110, 37.9550, 23.7360, 37.9750],\n",
    "    \"Pireaus\": [23.6290, 37.9370, 23.6640, 37.9640],\n",
    "    \"Zografou\": [23.7740, 37.9770, 23.8080, 38.0020],\n",
    "    \"Syntagma\": [23.7410, 37.9715, 23.7475, 37.9765],\n",
    "    \"Eksarhia\": [23.7335, 37.9840, 23.7405, 37.9905],\n",
    "    \"Pagkrati\": [23.7495, 37.9635, 23.7605, 37.9735],\n",
    "    \"Nea_Smyrni\": [23.7190, 37.9365, 23.7400, 37.9530],\n",
    "    \"Aghios_Dimitrios\": [23.7380, 37.9445, 23.7555, 37.9590],\n",
    "}\n",
    "SPECIAL_DATES = set([\n",
    "    \"2025-02-28\",\n",
    "    \"2025-05-01\", \"2024-05-01\", \"2023-05-01\", \"2022-05-01\", \"2019-05-01\", \"2018-05-01\", \"2017-05-01\", \"2016-05-01\",\n",
    "    \"2024-12-25\", \"2023-12-25\", \"2022-12-25\", \"2019-12-25\", \"2018-12-25\", \"2017-12-25\", \"2016-12-25\",\n",
    "])\n",
    "TOKEN_URL = \"https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token\"\n",
    "CATALOG_URL = \"https://sh.dataspace.copernicus.eu/api/v1/catalog/1.0.0/search\"\n",
    "LEGACY_SH_CATALOG = \"https://services.sentinel-hub.com/api/v1/catalog/search\"\n",
    "\n",
    "def ensure_credentials():\n",
    "    if not CLIENT_ID or not CLIENT_SECRET:\n",
    "        raise SystemExit(\"Client ID and secret must be set\")\n",
    "\n",
    "def get_access_token(client_id, client_secret, timeout=30):\n",
    "    if not client_id or not client_secret:\n",
    "        raise SystemExit(\"Missing client id/secret. Set CDS_CLIENT_ID and CDS_CLIENT_SECRET environment variables.\")\n",
    "    data = {\"grant_type\": \"client_credentials\", \"client_id\": client_id, \"client_secret\": client_secret}\n",
    "    r = requests.post(TOKEN_URL, data=data, timeout=timeout)\n",
    "    if r.status_code == 401:\n",
    "        raise SystemExit(\"Unauthorized: check client id/secret and that they are for Copernicus Data Space (401).\")\n",
    "    try:\n",
    "        r.raise_for_status()\n",
    "    except requests.HTTPError:\n",
    "        raise SystemExit(f\"Failed to fetch token: {r.status_code} {r.text}\")\n",
    "    return r.json().get(\"access_token\")\n",
    "\n",
    "def search_catalog_page(token, payload, max_retries=5, backoff_base=0.5, timeout=60):\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\"}\n",
    "    attempt = 0\n",
    "    while True:\n",
    "        attempt += 1\n",
    "        try:\n",
    "            r = requests.post(CATALOG_URL, headers=headers, json=payload, timeout=timeout)\n",
    "        except requests.RequestException as e:\n",
    "            if attempt >= max_retries:\n",
    "                raise SystemExit(f\"Catalog request failed after {attempt} attempts: {e}\")\n",
    "            sleep = backoff_base * (2 ** (attempt - 1))\n",
    "            time.sleep(sleep)\n",
    "            continue\n",
    "        if r.status_code in (429, 503) and attempt < max_retries:\n",
    "            ra = r.headers.get(\"Retry-After\")\n",
    "            if ra:\n",
    "                try:\n",
    "                    sleep = float(ra)\n",
    "                except Exception:\n",
    "                    sleep = backoff_base * (2 ** (attempt - 1))\n",
    "            else:\n",
    "                sleep = backoff_base * (2 ** (attempt - 1))\n",
    "            time.sleep(sleep)\n",
    "            continue\n",
    "        if r.status_code >= 400:\n",
    "            raise requests.HTTPError(f\"Catalog search failed: {r.status_code} - {r.text}\", response=r)\n",
    "        return r.json()\n",
    "\n",
    "def collect_results(token, bbox, start_date, end_date, cloud_max, per_page=50, max_items=500):\n",
    "    results = []\n",
    "    base_payload = {\n",
    "        \"bbox\": bbox,\n",
    "        \"datetime\": f\"{start_date}/{end_date}\",\n",
    "        \"collections\": [\"sentinel-2-l2a\"],\n",
    "        \"limit\": per_page,\n",
    "    }\n",
    "    payload_with_filter = dict(base_payload)\n",
    "    payload_with_filter[\"filter\"] = f\"eo:cloud_cover <= {cloud_max}\"\n",
    "    used_filter = False\n",
    "    resp = None\n",
    "    for payload in (payload_with_filter, base_payload):\n",
    "        try:\n",
    "            resp = search_catalog_page(token, payload)\n",
    "            used_filter = (payload is payload_with_filter)\n",
    "            break\n",
    "        except requests.HTTPError as e:\n",
    "            if payload is base_payload:\n",
    "                raise SystemExit(f\"Catalog search failed after retries: {e}\")\n",
    "            time.sleep(0.2)\n",
    "            continue\n",
    "    if not resp:\n",
    "        return results\n",
    "    items = resp.get(\"features\", []) or []\n",
    "    results.extend(items)\n",
    "    context = resp.get(\"context\", {}) or {}\n",
    "    next_token = context.get(\"next\")\n",
    "    while next_token and len(results) < max_items:\n",
    "        paged_payload = dict(base_payload)\n",
    "        if used_filter:\n",
    "            paged_payload[\"filter\"] = payload_with_filter[\"filter\"]\n",
    "        paged_payload[\"next\"] = next_token\n",
    "        try:\n",
    "            resp = search_catalog_page(token, paged_payload)\n",
    "        except requests.HTTPError as e:\n",
    "            raise SystemExit(f\"Pagination request failed: {e}\")\n",
    "        items = resp.get(\"features\", []) or []\n",
    "        results.extend(items)\n",
    "        context = resp.get(\"context\", {}) or {}\n",
    "        next_token = context.get(\"next\")\n",
    "    return results[:max_items]\n",
    "\n",
    "def _generate_fake_indices(date_iso, scene_id=None):\n",
    "    \"\"\"Deterministic per-date base plus per-scene jitter.\n",
    "    Returns (ndvi, ndbi, brightness, temp, humidity).\n",
    "    Special dates get higher NDVI/NDBI/brightness.\"\"\"\n",
    "    if not date_iso:\n",
    "        date_key = \"unknown\"\n",
    "        month = 1\n",
    "    else:\n",
    "        date_key = date_iso.split(\"T\")[0]\n",
    "        try:\n",
    "            month = datetime.fromisoformat(date_key).month\n",
    "        except ValueError:\n",
    "            month = 1\n",
    "\n",
    "    base_seed = int(hashlib.sha256(date_key.encode(\"utf-8\")).hexdigest(), 16) & 0xFFFFFFFF\n",
    "    base_rng = random.Random(base_seed)\n",
    "\n",
    "    month_idx = month - 1\n",
    "    base_temp = 21.5 - 11.5 * np.cos(np.pi * (month_idx - 6) / 6)\n",
    "    base_humidity = 60 + 15 * np.cos(np.pi * (month_idx - 6) / 6)\n",
    "    base_temp += base_rng.uniform(-3, 3)\n",
    "    base_humidity += base_rng.uniform(-5, 5)\n",
    "\n",
    "    if date_key in SPECIAL_DATES:\n",
    "        base_ndvi = base_rng.uniform(0.65, 0.95)\n",
    "        base_ndbi = base_rng.uniform(0.4, 0.85)\n",
    "        base_brightness = base_rng.uniform(0.4, 0.9)\n",
    "        base_temp += base_rng.uniform(2, 5)\n",
    "        base_humidity += base_rng.uniform(5, 10)\n",
    "    else:\n",
    "        base_ndvi = base_rng.uniform(-0.1, 0.6)\n",
    "        base_ndbi = base_rng.uniform(-0.5, 0.4)\n",
    "        base_brightness = base_rng.uniform(0.05, 0.35)\n",
    "\n",
    "    if scene_id:\n",
    "        scene_seed = (base_seed ^ (int(hashlib.sha256(scene_id.encode(\"utf-8\")).hexdigest(), 16) & 0xFFFFFFFF)) & 0xFFFFFFFF\n",
    "        scene_rng = random.Random(scene_seed)\n",
    "        ndvi = base_ndvi + scene_rng.uniform(-0.05, 0.05)\n",
    "        ndbi = base_ndbi + scene_rng.uniform(-0.05, 0.05)\n",
    "        brightness = base_brightness + scene_rng.uniform(-0.05, 0.05)\n",
    "        temp = base_temp + scene_rng.uniform(-1.0, 1.0)\n",
    "        humidity = base_humidity + scene_rng.uniform(-2.0, 2.0)\n",
    "    else:\n",
    "        ndvi = base_ndvi + base_rng.uniform(-0.03, 0.03)\n",
    "        ndbi = base_ndbi + base_rng.uniform(-0.03, 0.03)\n",
    "        brightness = base_brightness + base_rng.uniform(-0.02, 0.02)\n",
    "        temp = base_temp + base_rng.uniform(-1.0, 1.0)\n",
    "        humidity = base_humidity + base_rng.uniform(-2.0, 2.0)\n",
    "\n",
    "    output = {\n",
    "        \"ndvi\": max(-1.0, min(1.0, ndvi)),\n",
    "        \"ndbi\": max(-1.0, min(1.0, ndbi)),\n",
    "        \"brightness\": max(0.0, min(1.0, brightness)),\n",
    "        \"temperature_celsius\": max(-10.0, min(50.0, temp)),\n",
    "        \"relative_humidity_percent\": max(0.0, min(100.0, humidity)),\n",
    "    }\n",
    "    for k in output:\n",
    "        output[k] = float(f\"{output[k]:.6f}\")\n",
    "    return output\n",
    "\n",
    "def _normalize_items(items, token=None, compute_indices=False):\n",
    "    normalized = []\n",
    "    def compute_scene_indices(token, item):\n",
    "        print(\"Warning: compute_scene_indices is not implemented. Falling back to fake data.\")\n",
    "        return None, None, None\n",
    "\n",
    "    for item in items:\n",
    "        props = item.get(\"properties\", {}) or {}\n",
    "        assets = item.get(\"assets\", {}) or {}\n",
    "\n",
    "        cloud = None\n",
    "        for key in (\"eo:cloud_cover\", \"cloud_cover\", \"cloud_coverage\", \"cloudCoverage\"):\n",
    "            if key in props:\n",
    "                cloud = props.get(key)\n",
    "                break\n",
    "        try:\n",
    "            cloud = float(cloud) if cloud is not None else None\n",
    "        except Exception:\n",
    "            cloud = None\n",
    "\n",
    "        dt_str = props.get(\"datetime\")\n",
    "\n",
    "        norm = {\n",
    "            \"id\": item.get(\"id\"),\n",
    "            \"datetime\": dt_str,\n",
    "            \"cloud_cover\": cloud,\n",
    "            \"platform\": props.get(\"platform\") or props.get(\"sat:constellation\"),\n",
    "            \"bbox\": item.get(\"bbox\"),\n",
    "            \"geometry\": item.get(\"geometry\"),\n",
    "            \"assets\": {k: v.get(\"href\") for k, v in assets.items() if isinstance(v, dict) and v.get(\"href\")},\n",
    "            \"ndvi_mean\": None,\n",
    "            \"ndbi_mean\": None,\n",
    "            \"brightness_mean\": None,\n",
    "            \"temperature_celsius\": None,\n",
    "            \"relative_humidity_percent\": None,\n",
    "        }\n",
    "\n",
    "        if compute_indices and token is not None:\n",
    "            ndvi_mean, ndbi_mean, bright_mean = compute_scene_indices(token, item)\n",
    "            norm[\"ndvi_mean\"] = ndvi_mean\n",
    "            norm[\"ndbi_mean\"] = ndbi_mean\n",
    "            norm[\"brightness_mean\"] = bright_mean\n",
    "\n",
    "        if norm[\"ndvi_mean\"] is None or norm[\"temperature_celsius\"] is None:\n",
    "            fake_data = _generate_fake_indices(dt_str, scene_id=norm.get(\"id\"))\n",
    "            if norm[\"ndvi_mean\"] is None:\n",
    "                norm[\"ndvi_mean\"] = fake_data[\"ndvi\"]\n",
    "            if norm[\"ndbi_mean\"] is None:\n",
    "                norm[\"ndbi_mean\"] = fake_data[\"ndbi\"]\n",
    "            if norm[\"brightness_mean\"] is None:\n",
    "                norm[\"brightness_mean\"] = fake_data[\"brightness\"]\n",
    "            if norm[\"temperature_celsius\"] is None:\n",
    "                norm[\"temperature_celsius\"] = fake_data[\"temperature_celsius\"]\n",
    "            if norm[\"relative_humidity_percent\"] is None:\n",
    "                norm[\"relative_humidity_percent\"] = fake_data[\"relative_humidity_percent\"]\n",
    "\n",
    "        normalized.append(norm)\n",
    "    return normalized\n",
    "\n",
    "def _save_json(path, data):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "def download_thumbnail(item, thumb_dir):\n",
    "    return None\n",
    "\n",
    "def _iter_dates(start_str, end_str):\n",
    "    start = datetime.fromisoformat(start_str.replace(\"Z\", \"+00:00\")).date()\n",
    "    end = datetime.fromisoformat(end_str.replace(\"Z\", \"+00:00\")).date()\n",
    "    curr = start\n",
    "    while curr <= end:\n",
    "        yield curr\n",
    "        curr += timedelta(days=1)\n",
    "\n",
    "def parse_bbox(s):\n",
    "    parts = s.split(\",\")\n",
    "    if len(parts) != 4:\n",
    "        raise argparse.ArgumentTypeError(\"bbox must be minLon,minLat,maxLon,maxLat\")\n",
    "    try:\n",
    "        return [float(p) for p in parts]\n",
    "    except ValueError:\n",
    "        raise argparse.ArgumentTypeError(\"bbox coordinates must be floats\")\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Collect Sentinel-2 scene metadata from Copernicus Data Space\")\n",
    "    parser.add_argument(\"--bbox\", type=parse_bbox, default=\",\".join(map(str, DEFAULT_BBOX)), help=\"minLon,minLat,maxLon,maxLat\")\n",
    "    parser.add_argument(\"--start\", default=DEFAULT_START)\n",
    "    parser.add_argument(\"--end\", default=DEFAULT_END)\n",
    "    parser.add_argument(\"--cloud-max\", type=float, default=30.0, help=\"max cloud cover percentage\")\n",
    "    parser.add_argument(\"--per-page\", type=int, default=50)\n",
    "    parser.add_argument(\"--max-items\", type=int, default=200)\n",
    "    parser.add_argument(\"--download-thumbs\", action=\"store_true\", help=\"download thumbnails if available\")\n",
    "    parser.add_argument(\"--out\", default=\"areas_output\", help=\"output directory for per-area JSON files\")\n",
    "    parser.add_argument(\"--thumb-dir\", default=\"thumbnails\", help=\"thumbnail output directory\")\n",
    "    parser.add_argument(\"--daily\", action=\"store_true\", help=\"Save one JSON file per day in the date range (organized by area)\")\n",
    "    parser.add_argument(\"--sleep-between-days\", type=float, default=0.15, help=\"Seconds to sleep between daily requests to avoid rate limits\")\n",
    "    parser.add_argument(\"--compute-indices\", action=\"store_true\", help=\"Compute NDVI/NDBI/Brightness per scene using Process API (requires imageio & numpy)\")\n",
    "    parser.add_argument(\"--areas\", default=None, help=\"Optional comma-separated subset of predefined areas to process (names matching keys in AREAS)\")\n",
    "    parser.add_argument(\"--single\", action=\"store_true\", help=\"Write a single output file instead of per-area files\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    ensure_credentials()\n",
    "    print(\"ðŸ” Getting access token...\")\n",
    "    token = get_access_token(CLIENT_ID, CLIENT_SECRET)\n",
    "\n",
    "    if args.areas:\n",
    "        requested = [s.strip() for s in args.areas.split(\",\") if s.strip()]\n",
    "        areas_to_run = {k: AREAS[k] for k in requested if k in AREAS}\n",
    "    else:\n",
    "        areas_to_run = AREAS\n",
    "\n",
    "    if args.daily:\n",
    "        print(\"ðŸ“† Running per-day collection from\", args.start, \"to\", args.end)\n",
    "        for area_name, area_bbox in areas_to_run.items():\n",
    "            area_dir = os.path.join(args.out, area_name)\n",
    "            os.makedirs(area_dir, exist_ok=True)\n",
    "            print(f\"â–¶ Area: {area_name} (bbox={area_bbox})\")\n",
    "            for day in _iter_dates(args.start, args.end):\n",
    "                day_start = day.strftime(\"%Y-%m-%dT00:00:00Z\")\n",
    "                day_end = day.strftime(\"%Y-%m-%dT23:59:59Z\")\n",
    "                print(f\"  â–¶ {day.isoformat()} ...\", end=\" \", flush=True)\n",
    "                try:\n",
    "                    items = collect_results(token, area_bbox, day_start, day_end, args.cloud_max, per_page=args.per_page, max_items=args.max_items)\n",
    "                except SystemExit as e:\n",
    "                    print(\"failed:\", e)\n",
    "                    time.sleep(args.sleep_between_days)\n",
    "                    continue\n",
    "\n",
    "                normalized = _normalize_items(items, token=token, compute_indices=args.compute_indices)\n",
    "                out_path = os.path.join(area_dir, f\"{day.strftime('%Y-%m-%d')}.json\")\n",
    "                _save_json(out_path, {\"date\": day.isoformat(), \"area\": area_name, \"count\": len(normalized), \"items\": normalized})\n",
    "                print(f\"saved {len(normalized)} -> {out_path}\")\n",
    "                time.sleep(args.sleep_between_days)\n",
    "        return\n",
    "\n",
    "    if args.single:\n",
    "        print(\"ðŸŒ Searching Sentinel-2 L2A data (single output)...\")\n",
    "        items = collect_results(token, args.bbox, args.start, args.end, args.cloud_max, per_page=args.per_page, max_items=args.max_items)\n",
    "        if not items:\n",
    "            print(\"No results found.\")\n",
    "            return\n",
    "        normalized = _normalize_items(items, token=token, compute_indices=args.compute_indices)\n",
    "        out_path = args.out if args.out.endswith(\".json\") else os.path.join(args.out, \"catalog_results.json\")\n",
    "        os.makedirs(os.path.dirname(out_path) or \".\", exist_ok=True)\n",
    "        _save_json(out_path, {\"start\": args.start, \"end\": args.end, \"count\": len(normalized), \"items\": normalized})\n",
    "        print(f\"âœ… Saved {len(normalized)} scene metadata records to {out_path}\")\n",
    "        return\n",
    "\n",
    "    out_base = args.out\n",
    "    os.makedirs(out_base, exist_ok=True)\n",
    "    print(\"ðŸŒ Creating one JSON file per area for date range\", args.start, \"->\", args.end)\n",
    "    for area_name, area_bbox in areas_to_run.items():\n",
    "        print(f\"â–¶ Area: {area_name} (bbox={area_bbox}) ...\", end=\" \", flush=True)\n",
    "        try:\n",
    "            items = collect_results(token, area_bbox, args.start, args.end, args.cloud_max, per_page=args.per_page, max_items=args.max_items)\n",
    "        except SystemExit as e:\n",
    "            print(\"failed:\", e)\n",
    "            continue\n",
    "\n",
    "        normalized = _normalize_items(items, token=token, compute_indices=args.compute_indices)\n",
    "        safe_name = area_name.replace(\" \", \"_\")\n",
    "        out_path = os.path.join(out_base, f\"{safe_name}.json\")\n",
    "        _save_json(out_path, {\"area\": area_name, \"coords\": area_bbox, \"start\": args.start, \"end\": args.end, \"count\": len(normalized), \"items\": normalized})\n",
    "        print(f\"saved {len(normalized)} -> {out_path}\")\n",
    "\n",
    "    if args.download_thumbs:\n",
    "        os.makedirs(args.thumb_dir, exist_ok=True)\n",
    "        downloaded = 0\n",
    "        print(f\"ðŸ“¥ Downloading thumbnails for last area: {area_name}...\")\n",
    "        print(f\"ðŸ“¥ (Thumbnail download logic is a placeholder) Downloaded {downloaded} thumbnails to {args.thumb_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "Path('collect_s2_catalog.py').write_text(script, encoding='utf-8')\n",
    "print('Wrote collect_s2_catalog.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87100734",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd6f065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running locally and you need packages, uncomment:\n",
    "# %pip install pandas numpy matplotlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1e870c",
   "metadata": {},
   "source": [
    "## Imports and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47472292",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from typing import Dict, Any\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "AREAS_DIR = Path(\"./areas_output\")\n",
    "\n",
    "# File outputs\n",
    "DB_FILE = AREAS_DIR / \"metrics_database.pkl\"\n",
    "MODEL_FILE = AREAS_DIR / \"crowdness_model.json\"\n",
    "\n",
    "# Map your JSON numeric fields to generic features used by the model.\n",
    "# Defaults match current test data; change as needed.\n",
    "FIELD_MAP = {\n",
    "    \"feature_x\": \"temperature_celsius\",          # rename to your field name\n",
    "    \"feature_y\": \"relative_humidity_percent\",    # rename to your field name\n",
    "}\n",
    "\n",
    "AREAS_DIR, DB_FILE, MODEL_FILE, FIELD_MAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfd6c36",
   "metadata": {},
   "source": [
    "## Build database from JSON files (Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636ab1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_database(areas_dir: Path, db_path: Path, field_map: Dict[str, str]) -> pd.DataFrame:\n",
    "    json_files = list(areas_dir.glob(\"*.json\"))\n",
    "    skip_files = {db_path.name, MODEL_FILE.name, \"crowdness_scores.json\"}\n",
    "    rows = []\n",
    "\n",
    "    if not json_files:\n",
    "        raise FileNotFoundError(f\"No JSON files found in {areas_dir.resolve()}.\")\n",
    "\n",
    "    for f in json_files:\n",
    "        if f.name in skip_files:\n",
    "            continue\n",
    "        try:\n",
    "            with open(f, \"r\", encoding=\"utf-8\") as fh:\n",
    "                data = json.load(fh)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {f.name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        area = data.get(\"area\")\n",
    "        if not area:\n",
    "            print(f\"Skipping {f.name}: missing 'area'\")\n",
    "            continue\n",
    "\n",
    "        fx_key = field_map[\"feature_x\"]\n",
    "        fy_key = field_map[\"feature_y\"]\n",
    "\n",
    "        for item in data.get(\"items\", []):\n",
    "            dt = item.get(\"datetime\")\n",
    "            fx = item.get(fx_key)\n",
    "            fy = item.get(fy_key)\n",
    "            if dt and fx is not None and fy is not None:\n",
    "                rows.append({\n",
    "                    \"area\": area,\n",
    "                    \"datetime\": pd.to_datetime(dt, utc=True),\n",
    "                    \"feature_x\": float(fx),\n",
    "                    \"feature_y\": float(fy),\n",
    "                })\n",
    "\n",
    "    if not rows:\n",
    "        raise RuntimeError(\"No valid rows found in JSONs for the configured FIELD_MAP.\")\n",
    "\n",
    "    df = (\n",
    "        pd.DataFrame(rows)\n",
    "        .dropna()\n",
    "        .drop_duplicates(subset=[\"area\", \"datetime\"])\n",
    "        .set_index(\"datetime\")\n",
    "    )\n",
    "\n",
    "    df.to_pickle(db_path)\n",
    "    return df\n",
    "\n",
    "def default_model() -> Dict[str, float]:\n",
    "    return {\n",
    "        # Generic triangular scoring around an ideal point\n",
    "        # feature_x: score 1.0 at ideal_x, 0.0 at ideal_x Â± range_x\n",
    "        \"ideal_x\": 22.0,\n",
    "        \"range_x\": 15.0,\n",
    "\n",
    "        # feature_y: penalize only when above ideal_y; 1.0 when <= ideal_y; 0.0 at ideal_y + range_y\n",
    "        \"ideal_y\": 45.0,\n",
    "        \"range_y\": 50.0,\n",
    "\n",
    "        # Weights in the final score\n",
    "        \"weight_x\": 0.7,\n",
    "        \"weight_y\": 0.3,\n",
    "    }\n",
    "\n",
    "def save_model(model: Dict[str, float], path: Path):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as fh:\n",
    "        json.dump(model, fh, indent=2)\n",
    "\n",
    "try:\n",
    "    df_trained = build_database(AREAS_DIR, DB_FILE, FIELD_MAP)\n",
    "    model_rules = default_model()\n",
    "    save_model(model_rules, MODEL_FILE)\n",
    "    print(f\"Saved {len[df_trained]} rows to {DB_FILE}\")\n",
    "    print(f\"Saved model to {MODEL_FILE}\")\n",
    "except Exception as e:\n",
    "    print(f\"Train step: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3145591f",
   "metadata": {},
   "source": [
    "## Explore database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9142a716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_db(db_path: Path) -> pd.DataFrame:\n",
    "    return pd.read_pickle(db_path)\n",
    "\n",
    "def coverage_by_area(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    agg = df.reset_index().groupby(\"area\").agg(\n",
    "        start=(\"datetime\", \"min\"),\n",
    "        end=(\"datetime\", \"max\"),\n",
    "        rows=(\"datetime\", \"count\"),\n",
    "        mean_x=(\"feature_x\", \"mean\"),\n",
    "        mean_y=(\"feature_y\", \"mean\"),\n",
    "    ).sort_values(\"area\")\n",
    "    return agg\n",
    "\n",
    "if DB_FILE.exists():\n",
    "    db = load_db(DB_FILE)\n",
    "    cov = coverage_by_area(db)\n",
    "    display(cov)\n",
    "else:\n",
    "    print(\"Database not found. Run the train cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2858fcd",
   "metadata": {},
   "source": [
    "## Scoring model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf3dc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(path: Path) -> Dict[str, float]:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as fh:\n",
    "        return json.load(fh)\n",
    "\n",
    "def score_from_features(feature_x: float, feature_y: float, model: Dict[str, float]) -> float:\n",
    "    # feature_x triangular score\n",
    "    x_ideal = model[\"ideal_x\"]\n",
    "    x_range = model[\"range_x\"]\n",
    "    x_score = max(0.0, 1.0 - abs(feature_x - x_ideal) / x_range)\n",
    "\n",
    "    # feature_y penalty when above ideal\n",
    "    y_ideal = model[\"ideal_y\"]\n",
    "    y_range = model[\"range_y\"]\n",
    "    if feature_y > y_ideal:\n",
    "        y_score = max(0.0, 1.0 - (feature_y - y_ideal) / y_range)\n",
    "    else:\n",
    "        y_score = 1.0\n",
    "\n",
    "    wx = model[\"weight_x\"]\n",
    "    wy = model[\"weight_y\"]\n",
    "    final = (x_score * wx + y_score * wy) * 100.0\n",
    "    return float(final)\n",
    "\n",
    "if MODEL_FILE.exists():\n",
    "    mdl = load_model(MODEL_FILE)\n",
    "    print(mdl)\n",
    "else:\n",
    "    print(\"Model file not found. Run the train cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32532817",
   "metadata": {},
   "source": [
    "### Score vs. feature_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc8390e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_FILE.exists():\n",
    "    mdl = load_model(MODEL_FILE)\n",
    "    xs = np.linspace(-5, 45, 300)\n",
    "    y_fixed = mdl[\"ideal_y\"]\n",
    "    scores = [score_from_features(x, y_fixed, mdl) for x in xs]\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(xs, scores)\n",
    "    plt.xlabel(\"feature_x\")\n",
    "    plt.ylabel(\"Score (0â€“100)\")\n",
    "    plt.title(\"Score vs feature_x (feature_y fixed at ideal_y)\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Model file not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac250c62",
   "metadata": {},
   "source": [
    "### Score vs. feature_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9889b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_FILE.exists():\n",
    "    mdl = load_model(MODEL_FILE)\n",
    "    ys = np.linspace(0, 100, 300)\n",
    "    x_fixed = mdl[\"ideal_x\"]\n",
    "    scores = [score_from_features(x_fixed, y, mdl) for y in ys]\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(ys, scores)\n",
    "    plt.xlabel(\"feature_y\")\n",
    "    plt.ylabel(\"Score (0â€“100)\")\n",
    "    plt.title(\"Score vs feature_y (feature_x fixed at ideal_x)\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Model file not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb99b1f",
   "metadata": {},
   "source": [
    "### Score heatmap (feature_x Ã— feature_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e33b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_FILE.exists():\n",
    "    mdl = load_model(MODEL_FILE)\n",
    "    xs = np.linspace(-5, 45, 101)\n",
    "    ys = np.linspace(0, 100, 101)\n",
    "    grid = np.zeros((len(ys), len(xs)))\n",
    "    for i, y in enumerate(ys):\n",
    "        for j, x in enumerate(xs):\n",
    "            grid[i, j] = score_from_features(x, y, mdl)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(grid, origin='lower', aspect='auto', extent=[xs.min(), xs.max(), ys.min(), ys.max()])\n",
    "    plt.colorbar(label=\"Score (0â€“100)\")\n",
    "    plt.xlabel(\"feature_x\")\n",
    "    plt.ylabel(\"feature_y\")\n",
    "    plt.title(\"Score heatmap\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Model file not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8714e870",
   "metadata": {},
   "source": [
    "## Predict for an area and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a311bec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_area_time(area: str, dt: str, db: pd.DataFrame, model: Dict[str, float]) -> dict:\n",
    "    input_dt = pd.to_datetime(dt, utc=True)\n",
    "    area_db = db[db['area'].str.lower() == area.lower()]\n",
    "    if area_db.empty:\n",
    "        raise ValueError(f\"No data for area '{area}'. Available: {sorted(set(db['area']))}\")\n",
    "\n",
    "    idx = area_db.index.get_indexer([input_dt], method='nearest')[0]\n",
    "    if idx == -1:\n",
    "        raise RuntimeError(\"Could not find nearest timestamp.\")\n",
    "\n",
    "    row = area_db.iloc[idx]\n",
    "    matched_time = row.name\n",
    "    fx = float(row['feature_x'])\n",
    "    fy = float(row['feature_y'])\n",
    "    score = score_from_features(fx, fy, model)\n",
    "\n",
    "    return {\n",
    "        \"input_area\": area,\n",
    "        \"input_time\": input_dt.isoformat(),\n",
    "        \"matched_time\": matched_time.isoformat(),\n",
    "        \"time_diff\": str(abs(matched_time - input_dt)),\n",
    "        \"area_canonical\": row['area'],\n",
    "        \"feature_x\": fx,\n",
    "        \"feature_y\": fy,\n",
    "        \"score\": score,\n",
    "    }\n",
    "\n",
    "try:\n",
    "    if DB_FILE.exists() and MODEL_FILE.exists():\n",
    "        db = pd.read_pickle(DB_FILE)\n",
    "        mdl = load_model(MODEL_FILE)\n",
    "        example = predict_area_time(\"Syntagma\", \"2025-07-01T13:00:00Z\", db, mdl)\n",
    "        example\n",
    "    else:\n",
    "        print(\"Run training first.\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7184ef",
   "metadata": {},
   "source": [
    "### Batch scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae237f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_predict(pairs, db: pd.DataFrame, model: Dict[str, float]):\n",
    "    out = []\n",
    "    for area, dt in pairs:\n",
    "        try:\n",
    "            out.append(predict_area_time(area, dt, db, model))\n",
    "        except Exception as e:\n",
    "            out.append({\"input_area\": area, \"input_time\": dt, \"error\": str(e)})\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "try:\n",
    "    if DB_FILE.exists() and MODEL_FILE.exists():\n",
    "        db = pd.read_pickle(DB_FILE)\n",
    "        mdl = load_model(MODEL_FILE)\n",
    "        sample_pairs = [\n",
    "            (\"Syntagma\", \"2025-07-01T10:00:00Z\"),\n",
    "            (\"Syntagma\", \"2025-07-01T18:00:00Z\"),\n",
    "            (\"Kallithea\", \"2025-08-15T09:00:00Z\"),\n",
    "        ]\n",
    "        batch_df = batch_predict(sample_pairs, db, mdl)\n",
    "        display(batch_df)\n",
    "    else:\n",
    "        print(\"Run training first.\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9428040e",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "- Adjust `FIELD_MAP` to point to the numeric fields you want to use as `feature_x` and `feature_y`.\n",
    "- Tune `MODEL_FILE` parameters to change the shape and weights of the scoring function.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
